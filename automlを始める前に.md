# AutoMLを使った機械学習プロジェクトに入る前に
AutoML...データサイエンスを深く学んでいない人でも機械学習の簡単なところで使えるようになる素晴らしきサービス  
とはいえ __ある程度__ の前知識を身につけていないと十分にその恩恵を享受できなかったり，うっかりやっちゃダメなことをしてしまったりあるかも  
その __ある程度__ の勉強の足掛かりにしてもらえれば嬉しいです  
残念ながら私は構造化データしかやったことないので音声データや画像データについては別で気をつけるものがあるかも... やったら教えて

## 機械学習でやらなきゃいけないこと
ではそもそもAutoMLを使わない機械学習のプロジェクトとはどんな段階からなるのかというとざっくりと以下の段階がある

1. 要件の定義  
    予測対象は連続変数？それともカテゴリ変数？カテゴリ変数は２値分類それともそれ以上？  
    評価指標はどうする?  
    ここまでは機械学習そのものの要件.進めながらでもいいけど後述のシステムの要件も考えないとね

2. データの前処理  
    みんな大好き前処理. 現実世界は信じられないデータで溢れています...テーブル定義書ってなんだっけ，この外れ値なんで発生するの？  
    どのテーブルから変数を拝借するか, どう加工するか  

3. アルゴリズムの選定  
    大体ライブラリが揃っているので似たような事例を探して選ぼう．  
    そのまま使うも良し, 複数のアルゴリズムを自分なりにアンサンブルするも良し, 自分でアルゴリズム組んで実装するも良し  
    使うアルゴリズムによって適した前処理(欠損処理やスケーリング, 正則化が...)  

4. 学習とチューニング
    クロスバリデーションはやったほうがいい．ホールドアウトでバリデーションを作っておくのも必要  
    時系列が関係するものは要注意

5. システム化  
    全部を手動実行していてはめんどくさいですよね．ワークフローを使って前処理やバッチ処理をして自動化  
    そもそもデータベースは？API使う？出力先までどう届ける?どのサービスを組み合わせるか（クライドサービスなら運用にかかる料金も気になるところ）  
    バックアップとかどうしよう  

6. 保守  
    さてシステムができました！  
    データは刻々と変わります．商品価格帯は年々上昇しているから金額をカテゴリ化したらいつか見直さないと  
    webログを使っているならサイト構造の変化があったらおかしなことになるよね  
    そもそもずっと同じアルゴリズム使えるかな?  
    そんなふうにデータの状況やモデル自体がちゃんと動いてくれているかを随時確認していって,性能が悪化するようなら修正する必要があります

このうち，AutoMLがやってくれるのは 「2. データの前処理」の一部と限定的ではある「3. アルゴリズムの選定」後は基本的な「4. 学習とチューニング」  
親切なサービスなら「6. 保守」の監視までやってくれます  
[Vertex AI 表形式データの概要 | Google Cloud](https://cloud.google.com/vertex-ai/docs/tabular-data/overview?hl=ja)  

## じゃあAutoMLでやってくれないことは？
全部やってくれたらどんなにいいでしょうねー．GoogleやAmazone, OpenAIは頑張ってほしい  
ビジネス要件とプロジェクトの整合性をしっかり確かめましょう．コストはそれなりにかかります，機械学習でなくても単純なルールベースでもいいのでは？  
そんなことを今一度振り返ってみましょ  

AutoMLにはできないことの最たる部分は **データの準備** です，データ分析と同じで "Garbage In, Garbage Out." 入れるデータが残念だったら何にもならない  
データのクレンジングは徹底にガッツリやりましょ  
スケーリングや欠損値補充くらいはAutoMLで担ってくれる場合もあります（多段代入法を試してもいいけど結構手間 & アルゴリズムによって適切なものが変わってくるからまずはAutoMLに委ねよう） 
アンダーサンプリングもやってくれるAutoMLはある（ってか基本やってくれる）怪しければ一回やってみてもいいかも

機械学習のデータ準備で一番気をつけないといけないのは **リーケージ** です．簡単に言うと「カンニング」になります  
未来を予測するタスクであるため，予測時点で知り得ない情報をモデルに組み込んでしまうと, 学習時点では良い性能を発揮するけどいざ実装すると全く役に立たなくなる可能性も

他に気を払ったほうがいいのは用いる特徴量（説明変数）です  
単純に予測するのみのテーマなら使えるものはなんでも使うべき，でもその結果や特徴量の影響を見てビジネスに活かすとならなるべく操作可能な特徴量を選びましょ  

例えばユーザーの解約を予測する離反予測モデルを作ったとする.  
一番影響のある特徴量が「サポートセンターの対応満足度」であればより良いサポートのシステムを用意すれば離反を減らせるかも  
では「最近の日経平均株価の変動率」が一番影響あるとしたら？もうどうしようもない...  

だからといって有用な特徴量を省くのではなく  
積極的に企業の動きによって操作可能となる変数を組み込むことや，変数自体の意味を意識していくことが大事になってくると思います

また踏み込んだアルゴリズムはAutoMLは準備されていないことが多いです  
例えば階層ベイズモデルは自動で作ることはできませんでも，AutoMLをまずやってみようという段階では必要とされていないレベルなのでまずいいでしょう

### 参考サイトなど
易しめなので読んでほしい  
* [欠損値の補完に係る主な方法について | 総務省](https://www.soumu.go.jp/main_content/000741247.pdf)  
* [機械学習のリーケージについて考える | note](https://note.com/kenichiro/n/n2ff08344160a)  
* [Kaggleで勝つデータ分析の技術 第三章 特徴量の作成](https://amzn.asia/d/6oC1q5C) ← 竹中さんが持ってる
* [「Kaggleで勝つデータ分析の技術」から見る実用的な機械学習の知見 | Zenn](https://zenn.dev/y0/articles/5f76249012200a)
* [説明可能なAIとは | IBM](https://www.ibm.com/jp-ja/topics/explainable-ai)

読むのに気合いがいるし，すぐに必要になることは絶対にない
* [様々な多段代入法アルゴリズムの比較 | jstage](https://www.stat.go.jp/training/2kenkyu/ihou/71/pdf/2-2-713.pdf)
* [機械学習デザインパターン | O'REILLY](https://www.oreilly.co.jp//books/9784873119564/)

## データ準備以外で気をつけたいこと
**評価指標** はアルゴリズムに適したものをAutoMLは出してくれるけど，それを見てどう判断するかは人間に委ねられます  
どこに閾値を持っていくのか, いくつを超えたら良しとするのか. ひとまず数式を覚える必要はないですAutoMLが出してくれるから!! 
でもその意味は最低限把握しておくことをお勧めします  

**DHW構造** システム化するのです, 特徴量の抜き差しや移植が起こることは後々あるかも  
巨大データとなるとその処理だけでも結構なデータ量となります. 従量課金だったら尚更気にしたほうがいいですね.資本力で殴るなら話は別です  
データ分析同様に綺麗なDHWを作る，処理階層を作ることはとっても大事です  
ここら辺は中島さんに聞いてみるといいかも．私の引き継ぎで大変な目に遭っているだろうから

### 参考サイトなど
* [混同行列(Confusion Matrix) とは 〜 2 値分類の機械学習のクラス分類について | Qiita](https://qiita.com/TsutomuNakamura/items/a1a6a02cb9bb0dcbb37f)
* [帰分析の評価指標まとめ　～決定係数と紛らわしい相関係数の説明を添えて～ | Qiita](https://qiita.com/oki_kosuke/items/3934cd311fc805cafe81)
* [Shapを用いた機械学習モデルの解釈説明 | Qiita](https://qiita.com/shin_mura/items/cde01198552eda9146b7)

## その他
### メモはしっかり残しましょう
どう言う意図でその変数を選んだのか, なんで除外したのか, どうしてカテゴリ化したのか  
スコアが下がったから, 上がったから, 分布がどうだから, 特徴量の有用性の上位だから...そんなこと  
使わなければいけない変数がめっちゃ増えます（今私が作っている内の１つは130の変数を使ってます）  
だからメモをしっかり残しましょう．作り終わったら分析レポート同様に残しましょう  

* 前処理のテーブル定義書
* ワークフローを作るならその定義書
* モデル概要（どんな特徴量を使っているか, 抽出期間は, なぜ入れたか, 作成時のスコアはどうだったか, 作成時の特徴量の分布はどうだったか）

### 特徴量の選択方法
解釈可能なモデルを目指すために，操作可能な意味のある特徴量を入れるってのとは別に  
統計局やデータクリーンルームから自社データ以外も使えるなら使おう（ユーザー属性に都道府県がはいるならその都道府県の情報，例えば平均収入や通勤時間など）  

特徴量の有用性などAutoMLの結果の一つとして出てきます（意味のある特徴量ランキングみたいな）  
そこからの選び方として __ランダムな数字を持つ変数を入れておく，それより明らかに低ければ除外する__ みたいな方法は使ってます

欠損が多いなどデータがスパース（希薄）な場合は除外する方が安牌です，他にないなら入れるけど優先順位は下げて良いでしょう  
50%程度の欠損なら使うかな．他に使える変数の量にもよるけども  
現実のデータは欠損が多いので，欠損があるからとレコード自体を除去するなどはやめましょう  
これは「欠損データの扱いについて学習させるため」です

### 資料など
* [統計局](https://www.stat.go.jp/)